{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "def _init_spark():\n",
    "    spark = SparkSession.builder.appName(\"2TaskSparkSQL\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    return spark, sc\n",
    "\n",
    "\n",
    "def load_file(filename=\"text.txt\", spark=None, sc=None):\n",
    "    \n",
    "    RDD = sc.textFile(filename)\n",
    "    \n",
    "    header = ['project_name', 'page_title', 'num_requests', 'content_size']\n",
    "    in_df = RDD.map(lambda x: x.split(\" \"))\n",
    "    df2 = in_df.toDF(header)\n",
    "    \n",
    "    DF = df2.withColumn(\"num_requests\", df2[\"num_requests\"].cast(LongType())).withColumn(\"content_size\", df2[\"content_size\"].cast(LongType()))\n",
    "    DF.printSchema()\n",
    "    print(DF.show(10))\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- project_name: string (nullable = true)\n",
      " |-- page_title: string (nullable = true)\n",
      " |-- num_requests: long (nullable = true)\n",
      " |-- content_size: long (nullable = true)\n",
      "\n",
      "+------------+--------------------+------------+------------+\n",
      "|project_name|          page_title|num_requests|content_size|\n",
      "+------------+--------------------+------------+------------+\n",
      "|          aa|%27Ir%C2%B7r%C2%B...|           1|           1|\n",
      "|          aa| Enqlizxsh_-_English|           1|           1|\n",
      "|          aa|           Main_Page|           2|           2|\n",
      "|          aa|Special:Recentcha...|           1|           1|\n",
      "|          aa|  Special:Statistics|           1|           1|\n",
      "|          aa|        User:JAnDbot|           1|           1|\n",
      "|          aa|          User:Koavf|           1|           1|\n",
      "|          aa|        User:Purbo_T|           1|           1|\n",
      "|          aa|         User:SieBot|           1|           1|\n",
      "|          aa|       User:Siebrand|           1|           1|\n",
      "+------------+--------------------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    spark, sc = _init_spark()\n",
    "    DF = load_file(\"data/pagecounts-20080301-000000\", spark, sc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "1\n",
      "2896526\n",
      "2\n",
      "267\n",
      "3\n",
      "6862122\n",
      "4\n",
      "[('Special:Search', 648915), ('Main_Page', 197237), ('Special:Random', 71862), ('Leap_year', 11936), ('Ricin', 7531)]\n"
     ]
    }
   ],
   "source": [
    "    \"\"\"  \n",
    "    SQL and DataSet API\n",
    "    1. Total number of elements.\n",
    "    2. Complete list of project names (no repetitions).\n",
    "    3. Total content size of project “en” (Wikipedia in English).\n",
    "    4. Top 5 most visited pages of project “en”, and the number of visits for each.\n",
    "    \"\"\"\n",
    "    print(type(DF))\n",
    "    print(1)\n",
    "    print(DF.count())\n",
    "    \n",
    "    proj_names = [x['project_name'] for x in DF.select('project_name').distinct().collect()]\n",
    "    print(2)\n",
    "    print(len(proj_names))\n",
    "    \n",
    "    numb_en_proj = DF.filter(DF[\"project_name\"] == \"en\").groupBy().sum().collect()\n",
    "    print(3)\n",
    "    print(numb_en_proj[0][1])\n",
    "    \n",
    "    df3 = DF.filter(DF[\"project_name\"] == \"en\")\n",
    "    df4 = df3.sort(\"num_requests\", ascending=False).take(5)\n",
    "    top_5 = [(line[\"page_title\"], line[\"num_requests\"]) for line in df4]\n",
    "    print(4)\n",
    "    print(top_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.createOrReplaceTempView(\"Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql1 = spark.sql(\"SELECT count(*) FROM Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2896526"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql1.first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql2 = spark.sql(\"SELECT DISTINCT project_name FROM Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql3 = spark.sql(\"SELECT SUM(content_size) FROM Wikipedia WHERE project_name == 'en' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6862122"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql3.first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql4 = spark.sql(\"SELECT page_title, num_requests FROM Wikipedia WHERE project_name =='en' ORDER BY num_requests DESC LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|          page_title|num_requests|\n",
      "+--------------------+------------+\n",
      "|      Special:Search|      648915|\n",
      "|           Main_Page|      197237|\n",
      "|      Special:Random|       71862|\n",
      "|           Leap_year|       11936|\n",
      "|               Ricin|        7531|\n",
      "| Canine_reproduction|        7115|\n",
      "|   Special:Watchlist|        6818|\n",
      "|                Wiki|        5614|\n",
      "|The_Rock_(enterta...|        3080|\n",
      "|        Barack_Obama|        2868|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
